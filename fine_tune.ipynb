{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Κατασκευή train και val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.71s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.36s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import pyvww\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "training_set = pyvww.pytorch.VisualWakeWordsClassification(\n",
    "    root=\"./visualwakewords/path-to-COCO-dataset/train2014\",\n",
    "    annFile=\"./visualwakewords/new-path-to-visualwakewords-dataset/annotations/instances_train.json\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "validation_set = pyvww.pytorch.VisualWakeWordsClassification(\n",
    "    root=\"./visualwakewords/path-to-COCO-dataset/train2014\",\n",
    "    annFile=\"./visualwakewords/new-path-to-visualwakewords-dataset/annotations/instances_val.json\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=256, shuffle=True, num_workers=4, pin_memory=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loss_fn, epoch):\n",
    "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(training_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "        if batch_idx % 25 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}] | Training Batch [{batch_idx+1}/{len(training_loader)}] | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return train_loss, train_correct, train_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_val(model, optimizer, loss_fn, epochs, log_file_path, model_file_path):\n",
    "    model.to(device)\n",
    "    best_vacc = 0\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    print(f\"Model is running on: {device}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}:')\n",
    "\n",
    "        # Training Phase\n",
    "        model.train() \n",
    "        train_loss, train_correct, train_total = train_one_epoch(model, optimizer, loss_fn, epoch)\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        train_acc = train_correct / train_total\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval() \n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (vinputs, vlabels) in enumerate(validation_loader):\n",
    "                vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "                val_loss += vloss.item() * vinputs.size(0)\n",
    "\n",
    "                _, vpreds = torch.max(voutputs, 1)\n",
    "                val_correct += (vpreds == vlabels).sum().item()\n",
    "                val_total += vlabels.size(0)\n",
    "                \n",
    "                if batch_idx % 25 == 0:  # Print every 10 batches\n",
    "                    print(f\"Epoch [{epoch+1}] | Validation Batch [{batch_idx+1}/{len(training_loader)}] | Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_acc = val_correct / val_total\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        with open(log_file_path, \"a\") as log_file:\n",
    "            if os.stat(log_file_path).st_size == 0:\n",
    "                log_file.write(\"Epoch\\tTrain_Loss\\tTrain_Acc\\tVal_Loss\\tVal_Acc\\n\")\n",
    "            \n",
    "            log_file.write(f\"{epoch + 1}\\t{train_loss:.4f}\\t{train_acc:.4f}\\t{val_loss:.4f}\\t{val_acc:.4f}\\n\")\n",
    "\n",
    "        # Save model if it's the best validation loss so far\n",
    "        if val_acc > best_vacc:\n",
    "            best_vacc = val_acc\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            print(\"Saved model with best performance so far.\")\n",
    "\n",
    "    return train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is running on: cuda\n",
      "Epoch 1:\n",
      "Model is running on: cuda\n",
      "Epoch 1:\n",
      "Epoch [1] | Training Batch [1/451] | Loss: 0.6947\n",
      "Epoch [1] | Training Batch [1/451] | Loss: 0.6951\n",
      "Epoch [1] | Training Batch [26/451] | Loss: 0.6917\n",
      "Epoch [1] | Training Batch [26/451] | Loss: 0.6943\n",
      "Epoch [1] | Training Batch [51/451] | Loss: 0.6917\n",
      "Epoch [1] | Training Batch [51/451] | Loss: 0.6904\n",
      "Epoch [1] | Training Batch [76/451] | Loss: 0.6907\n",
      "Epoch [1] | Training Batch [76/451] | Loss: 0.6915\n",
      "Epoch [1] | Training Batch [101/451] | Loss: 0.6912\n",
      "Epoch [1] | Training Batch [101/451] | Loss: 0.6903\n",
      "Epoch [1] | Training Batch [126/451] | Loss: 0.6879\n",
      "Epoch [1] | Training Batch [126/451] | Loss: 0.6892\n",
      "Epoch [1] | Training Batch [151/451] | Loss: 0.6889\n",
      "Epoch [1] | Training Batch [151/451] | Loss: 0.6891\n",
      "Epoch [1] | Training Batch [176/451] | Loss: 0.6879\n",
      "Epoch [1] | Training Batch [176/451] | Loss: 0.6886\n",
      "Epoch [1] | Training Batch [201/451] | Loss: 0.6862\n",
      "Epoch [1] | Training Batch [201/451] | Loss: 0.6861\n",
      "Epoch [1] | Training Batch [226/451] | Loss: 0.6864\n",
      "Epoch [1] | Training Batch [226/451] | Loss: 0.6876\n",
      "Epoch [1] | Training Batch [251/451] | Loss: 0.6848\n",
      "Epoch [1] | Training Batch [251/451] | Loss: 0.6839\n",
      "Epoch [1] | Training Batch [276/451] | Loss: 0.6832\n",
      "Epoch [1] | Training Batch [276/451] | Loss: 0.6863\n",
      "Epoch [1] | Training Batch [301/451] | Loss: 0.6834\n",
      "Epoch [1] | Training Batch [301/451] | Loss: 0.6809\n",
      "Epoch [1] | Training Batch [326/451] | Loss: 0.6772\n",
      "Epoch [1] | Training Batch [326/451] | Loss: 0.6810\n",
      "Epoch [1] | Training Batch [351/451] | Loss: 0.6814\n",
      "Epoch [1] | Training Batch [351/451] | Loss: 0.6815\n",
      "Epoch [1] | Training Batch [376/451] | Loss: 0.6863\n",
      "Epoch [1] | Training Batch [376/451] | Loss: 0.6795\n",
      "Epoch [1] | Training Batch [401/451] | Loss: 0.6798\n",
      "Epoch [1] | Training Batch [401/451] | Loss: 0.6842\n",
      "Epoch [1] | Training Batch [426/451] | Loss: 0.6828\n",
      "Epoch [1] | Training Batch [426/451] | Loss: 0.6796\n",
      "Epoch [1] | Training Batch [451/451] | Loss: 0.6814\n",
      "Epoch [1] | Validation Batch [1/451] | Loss: 173.2437\n",
      "Epoch [1] | Training Batch [451/451] | Loss: 0.6754\n",
      "Epoch [1] | Validation Batch [1/451] | Loss: 173.9151\n",
      "Epoch [1] | Validation Batch [26/451] | Loss: 4510.5237\n",
      "Train Acc: 0.5347 | Val Acc: 0.5569\n",
      "Saved model with best performance so far.\n",
      "Epoch 2:\n",
      "Epoch [1] | Validation Batch [26/451] | Loss: 4516.6050\n",
      "Epoch [2] | Training Batch [1/451] | Loss: 0.6798\n",
      "Train Acc: 0.5717 | Val Acc: 0.5867\n",
      "Saved model with best performance so far.\n",
      "Epoch 2:\n",
      "Epoch [2] | Training Batch [1/451] | Loss: 0.6799\n",
      "Epoch [2] | Training Batch [26/451] | Loss: 0.6762\n",
      "Epoch [2] | Training Batch [26/451] | Loss: 0.6783\n",
      "Epoch [2] | Training Batch [51/451] | Loss: 0.6795\n",
      "Epoch [2] | Training Batch [51/451] | Loss: 0.6778\n",
      "Epoch [2] | Training Batch [76/451] | Loss: 0.6732\n",
      "Epoch [2] | Training Batch [76/451] | Loss: 0.6774\n",
      "Epoch [2] | Training Batch [101/451] | Loss: 0.6781\n",
      "Epoch [2] | Training Batch [101/451] | Loss: 0.6752\n",
      "Epoch [2] | Training Batch [126/451] | Loss: 0.6722\n",
      "Epoch [2] | Training Batch [126/451] | Loss: 0.6747\n",
      "Epoch [2] | Training Batch [151/451] | Loss: 0.6753\n",
      "Epoch [2] | Training Batch [151/451] | Loss: 0.6742\n",
      "Epoch [2] | Training Batch [176/451] | Loss: 0.6681\n",
      "Epoch [2] | Training Batch [176/451] | Loss: 0.6712\n",
      "Epoch [2] | Training Batch [201/451] | Loss: 0.6679\n",
      "Epoch [2] | Training Batch [201/451] | Loss: 0.6688\n",
      "Epoch [2] | Training Batch [226/451] | Loss: 0.6663\n",
      "Epoch [2] | Training Batch [226/451] | Loss: 0.6729\n",
      "Epoch [2] | Training Batch [251/451] | Loss: 0.6669\n",
      "Epoch [2] | Training Batch [251/451] | Loss: 0.6711\n",
      "Epoch [2] | Training Batch [276/451] | Loss: 0.6716\n",
      "Epoch [2] | Training Batch [276/451] | Loss: 0.6665\n",
      "Epoch [2] | Training Batch [301/451] | Loss: 0.6685\n",
      "Epoch [2] | Training Batch [301/451] | Loss: 0.6701\n",
      "Epoch [2] | Training Batch [326/451] | Loss: 0.6697\n",
      "Epoch [2] | Training Batch [326/451] | Loss: 0.6680\n",
      "Epoch [2] | Training Batch [351/451] | Loss: 0.6701\n",
      "Epoch [2] | Training Batch [351/451] | Loss: 0.6690\n",
      "Epoch [2] | Training Batch [376/451] | Loss: 0.6656\n",
      "Epoch [2] | Training Batch [376/451] | Loss: 0.6695\n",
      "Epoch [2] | Training Batch [401/451] | Loss: 0.6615\n",
      "Epoch [2] | Training Batch [401/451] | Loss: 0.6693\n",
      "Epoch [2] | Training Batch [426/451] | Loss: 0.6640\n",
      "Epoch [2] | Training Batch [426/451] | Loss: 0.6685\n",
      "Epoch [2] | Training Batch [451/451] | Loss: 0.6601\n",
      "Epoch [2] | Validation Batch [1/451] | Loss: 169.0471\n",
      "Epoch [2] | Training Batch [451/451] | Loss: 0.6792\n",
      "Epoch [2] | Validation Batch [1/451] | Loss: 170.4082\n",
      "Epoch [2] | Validation Batch [26/451] | Loss: 4411.0107\n",
      "Train Acc: 0.6619 | Val Acc: 0.7439\n",
      "Saved model with best performance so far.\n",
      "Epoch 3:\n",
      "Epoch [2] | Validation Batch [26/451] | Loss: 4429.2812\n",
      "Train Acc: 0.6695 | Val Acc: 0.7000\n",
      "Saved model with best performance so far.\n",
      "Epoch 3:\n",
      "Epoch [3] | Training Batch [1/451] | Loss: 0.6614\n",
      "Epoch [3] | Training Batch [1/451] | Loss: 0.6614\n",
      "Epoch [3] | Training Batch [26/451] | Loss: 0.6628\n",
      "Epoch [3] | Training Batch [26/451] | Loss: 0.6639\n",
      "Epoch [3] | Training Batch [51/451] | Loss: 0.6585\n",
      "Epoch [3] | Training Batch [51/451] | Loss: 0.6633\n",
      "Epoch [3] | Training Batch [76/451] | Loss: 0.6608\n",
      "Epoch [3] | Training Batch [76/451] | Loss: 0.6627\n",
      "Epoch [3] | Training Batch [101/451] | Loss: 0.6589\n",
      "Epoch [3] | Training Batch [101/451] | Loss: 0.6625\n",
      "Epoch [3] | Training Batch [126/451] | Loss: 0.6650\n",
      "Epoch [3] | Training Batch [126/451] | Loss: 0.6569\n",
      "Epoch [3] | Training Batch [151/451] | Loss: 0.6538\n",
      "Epoch [3] | Training Batch [151/451] | Loss: 0.6591\n",
      "Epoch [3] | Training Batch [176/451] | Loss: 0.6601\n",
      "Epoch [3] | Training Batch [176/451] | Loss: 0.6565\n",
      "Epoch [3] | Training Batch [201/451] | Loss: 0.6609\n",
      "Epoch [3] | Training Batch [201/451] | Loss: 0.6603\n",
      "Epoch [3] | Training Batch [226/451] | Loss: 0.6541\n",
      "Epoch [3] | Training Batch [226/451] | Loss: 0.6553\n",
      "Epoch [3] | Training Batch [251/451] | Loss: 0.6554\n",
      "Epoch [3] | Training Batch [251/451] | Loss: 0.6603\n",
      "Epoch [3] | Training Batch [276/451] | Loss: 0.6509\n",
      "Epoch [3] | Training Batch [276/451] | Loss: 0.6594\n",
      "Epoch [3] | Training Batch [301/451] | Loss: 0.6462\n",
      "Epoch [3] | Training Batch [301/451] | Loss: 0.6584\n",
      "Epoch [3] | Training Batch [326/451] | Loss: 0.6488\n",
      "Epoch [3] | Training Batch [326/451] | Loss: 0.6548\n",
      "Epoch [3] | Training Batch [351/451] | Loss: 0.6568\n",
      "Epoch [3] | Training Batch [351/451] | Loss: 0.6542\n",
      "Epoch [3] | Training Batch [376/451] | Loss: 0.6552\n",
      "Epoch [3] | Training Batch [376/451] | Loss: 0.6491\n",
      "Epoch [3] | Training Batch [401/451] | Loss: 0.6564\n",
      "Epoch [3] | Training Batch [401/451] | Loss: 0.6532\n",
      "Epoch [3] | Training Batch [426/451] | Loss: 0.6589\n",
      "Epoch [3] | Training Batch [426/451] | Loss: 0.6489\n",
      "Epoch [3] | Training Batch [451/451] | Loss: 0.6601\n",
      "Epoch [3] | Training Batch [451/451] | Loss: 0.6605\n",
      "Epoch [3] | Validation Batch [1/451] | Loss: 167.1418\n",
      "Epoch [3] | Validation Batch [1/451] | Loss: 165.0925\n",
      "Epoch [3] | Validation Batch [26/451] | Loss: 4318.0014\n",
      "Epoch [3] | Validation Batch [26/451] | Loss: 4348.9666\n",
      "Train Acc: 0.7650 | Val Acc: 0.7769\n",
      "Train Acc: 0.7514 | Val Acc: 0.7755\n",
      "Saved model with best performance so far.\n",
      "Epoch 4:\n",
      "Saved model with best performance so far.\n",
      "Epoch 4:\n",
      "Epoch [4] | Training Batch [1/451] | Loss: 0.6479\n",
      "Epoch [4] | Training Batch [1/451] | Loss: 0.6479\n",
      "Epoch [4] | Training Batch [26/451] | Loss: 0.6514\n",
      "Epoch [4] | Training Batch [26/451] | Loss: 0.6492\n",
      "Epoch [4] | Training Batch [51/451] | Loss: 0.6456\n",
      "Epoch [4] | Training Batch [51/451] | Loss: 0.6463\n",
      "Epoch [4] | Training Batch [76/451] | Loss: 0.6509\n",
      "Epoch [4] | Training Batch [76/451] | Loss: 0.6471\n",
      "Epoch [4] | Training Batch [101/451] | Loss: 0.6418\n",
      "Epoch [4] | Training Batch [101/451] | Loss: 0.6518\n",
      "Epoch [4] | Training Batch [126/451] | Loss: 0.6433\n",
      "Epoch [4] | Training Batch [126/451] | Loss: 0.6474\n",
      "Epoch [4] | Training Batch [151/451] | Loss: 0.6461\n",
      "Epoch [4] | Training Batch [151/451] | Loss: 0.6402\n",
      "Epoch [4] | Training Batch [176/451] | Loss: 0.6538\n",
      "Epoch [4] | Training Batch [176/451] | Loss: 0.6415\n",
      "Epoch [4] | Training Batch [201/451] | Loss: 0.6438\n",
      "Epoch [4] | Training Batch [201/451] | Loss: 0.6493\n",
      "Epoch [4] | Training Batch [226/451] | Loss: 0.6477\n",
      "Epoch [4] | Training Batch [226/451] | Loss: 0.6485\n",
      "Epoch [4] | Training Batch [251/451] | Loss: 0.6440\n",
      "Epoch [4] | Training Batch [251/451] | Loss: 0.6458\n",
      "Epoch [4] | Training Batch [276/451] | Loss: 0.6464\n",
      "Epoch [4] | Training Batch [276/451] | Loss: 0.6413\n",
      "Epoch [4] | Training Batch [301/451] | Loss: 0.6424\n",
      "Epoch [4] | Training Batch [301/451] | Loss: 0.6380\n",
      "Epoch [4] | Training Batch [326/451] | Loss: 0.6440\n",
      "Epoch [4] | Training Batch [326/451] | Loss: 0.6332\n",
      "Epoch [4] | Training Batch [351/451] | Loss: 0.6367\n",
      "Epoch [4] | Training Batch [351/451] | Loss: 0.6376\n",
      "Epoch [4] | Training Batch [376/451] | Loss: 0.6466\n",
      "Epoch [4] | Training Batch [376/451] | Loss: 0.6383\n",
      "Epoch [4] | Training Batch [401/451] | Loss: 0.6444\n",
      "Epoch [4] | Training Batch [401/451] | Loss: 0.6416\n",
      "Epoch [4] | Training Batch [426/451] | Loss: 0.6413\n",
      "Epoch [4] | Training Batch [426/451] | Loss: 0.6378\n",
      "Epoch [4] | Training Batch [451/451] | Loss: 0.6256\n",
      "Epoch [4] | Training Batch [451/451] | Loss: 0.6575\n",
      "Epoch [4] | Validation Batch [1/451] | Loss: 161.5586\n",
      "Epoch [4] | Validation Batch [1/451] | Loss: 163.9085\n",
      "Epoch [4] | Validation Batch [26/451] | Loss: 4233.2258\n",
      "Epoch [4] | Validation Batch [26/451] | Loss: 4268.5698\n",
      "Train Acc: 0.7928 | Val Acc: 0.7945\n",
      "Train Acc: 0.7979 | Val Acc: 0.8061\n",
      "Saved model with best performance so far.\n",
      "Epoch 5:\n",
      "Saved model with best performance so far.\n",
      "Epoch 5:\n",
      "Epoch [5] | Training Batch [1/451] | Loss: 0.6333\n",
      "Epoch [5] | Training Batch [1/451] | Loss: 0.6398\n",
      "Epoch [5] | Training Batch [26/451] | Loss: 0.6458\n",
      "Epoch [5] | Training Batch [26/451] | Loss: 0.6249\n",
      "Epoch [5] | Training Batch [51/451] | Loss: 0.6445\n",
      "Epoch [5] | Training Batch [51/451] | Loss: 0.6277\n",
      "Epoch [5] | Training Batch [76/451] | Loss: 0.6473\n",
      "Epoch [5] | Training Batch [76/451] | Loss: 0.6332\n",
      "Epoch [5] | Training Batch [101/451] | Loss: 0.6381\n",
      "Epoch [5] | Training Batch [101/451] | Loss: 0.6246\n",
      "Epoch [5] | Training Batch [126/451] | Loss: 0.6358\n",
      "Epoch [5] | Training Batch [126/451] | Loss: 0.6284\n",
      "Epoch [5] | Training Batch [151/451] | Loss: 0.6317\n",
      "Epoch [5] | Training Batch [151/451] | Loss: 0.6340\n",
      "Epoch [5] | Training Batch [176/451] | Loss: 0.6341\n",
      "Epoch [5] | Training Batch [176/451] | Loss: 0.6300\n",
      "Epoch [5] | Training Batch [201/451] | Loss: 0.6358\n",
      "Epoch [5] | Training Batch [201/451] | Loss: 0.6238\n",
      "Epoch [5] | Training Batch [226/451] | Loss: 0.6246\n",
      "Epoch [5] | Training Batch [226/451] | Loss: 0.6266\n",
      "Epoch [5] | Training Batch [251/451] | Loss: 0.6230\n",
      "Epoch [5] | Training Batch [251/451] | Loss: 0.6449\n",
      "Epoch [5] | Training Batch [276/451] | Loss: 0.6255\n",
      "Epoch [5] | Training Batch [276/451] | Loss: 0.6277\n",
      "Epoch [5] | Training Batch [301/451] | Loss: 0.6305\n",
      "Epoch [5] | Training Batch [301/451] | Loss: 0.6320\n",
      "Epoch [5] | Training Batch [326/451] | Loss: 0.6298\n",
      "Epoch [5] | Training Batch [326/451] | Loss: 0.6348\n",
      "Epoch [5] | Training Batch [351/451] | Loss: 0.6246\n",
      "Epoch [5] | Training Batch [351/451] | Loss: 0.6378\n",
      "Epoch [5] | Training Batch [376/451] | Loss: 0.6151\n",
      "Epoch [5] | Training Batch [376/451] | Loss: 0.6264\n",
      "Epoch [5] | Training Batch [401/451] | Loss: 0.6247\n",
      "Epoch [5] | Training Batch [401/451] | Loss: 0.6294\n",
      "Epoch [5] | Training Batch [426/451] | Loss: 0.6280\n",
      "Epoch [5] | Training Batch [426/451] | Loss: 0.6256\n",
      "Epoch [5] | Training Batch [451/451] | Loss: 0.6280\n",
      "Epoch [5] | Validation Batch [1/451] | Loss: 158.1247\n",
      "Epoch [5] | Training Batch [451/451] | Loss: 0.6176\n",
      "Epoch [5] | Validation Batch [1/451] | Loss: 160.7908\n",
      "Epoch [5] | Validation Batch [26/451] | Loss: 4152.3717\n",
      "Epoch [5] | Validation Batch [26/451] | Loss: 4192.0403\n",
      "Train Acc: 0.8031 | Val Acc: 0.8073\n",
      "Saved model with best performance so far.\n",
      "Epoch 6:\n",
      "Train Acc: 0.8181 | Val Acc: 0.8257\n",
      "Saved model with best performance so far.\n",
      "Epoch 6:\n",
      "Epoch [6] | Training Batch [1/451] | Loss: 0.6340\n",
      "Epoch [6] | Training Batch [1/451] | Loss: 0.6280\n",
      "Epoch [6] | Training Batch [26/451] | Loss: 0.6233\n",
      "Epoch [6] | Training Batch [26/451] | Loss: 0.6182\n",
      "Epoch [6] | Training Batch [51/451] | Loss: 0.6278\n",
      "Epoch [6] | Training Batch [51/451] | Loss: 0.6251\n",
      "Epoch [6] | Training Batch [76/451] | Loss: 0.6228\n",
      "Epoch [6] | Training Batch [76/451] | Loss: 0.6273\n",
      "Epoch [6] | Training Batch [101/451] | Loss: 0.6226\n",
      "Epoch [6] | Training Batch [101/451] | Loss: 0.6286\n",
      "Epoch [6] | Training Batch [126/451] | Loss: 0.6149\n",
      "Epoch [6] | Training Batch [126/451] | Loss: 0.6218\n",
      "Epoch [6] | Training Batch [151/451] | Loss: 0.6193\n",
      "Epoch [6] | Training Batch [151/451] | Loss: 0.6228\n",
      "Epoch [6] | Training Batch [176/451] | Loss: 0.6095\n",
      "Epoch [6] | Training Batch [176/451] | Loss: 0.6247\n",
      "Epoch [6] | Training Batch [201/451] | Loss: 0.6255\n",
      "Epoch [6] | Training Batch [201/451] | Loss: 0.6241\n",
      "Epoch [6] | Training Batch [226/451] | Loss: 0.6195\n",
      "Epoch [6] | Training Batch [226/451] | Loss: 0.6133\n",
      "Epoch [6] | Training Batch [251/451] | Loss: 0.6231\n",
      "Epoch [6] | Training Batch [251/451] | Loss: 0.6053\n",
      "Epoch [6] | Training Batch [276/451] | Loss: 0.6234\n",
      "Epoch [6] | Training Batch [276/451] | Loss: 0.6173\n",
      "Epoch [6] | Training Batch [301/451] | Loss: 0.6157\n",
      "Epoch [6] | Training Batch [301/451] | Loss: 0.6118\n",
      "Epoch [6] | Training Batch [326/451] | Loss: 0.6167\n",
      "Epoch [6] | Training Batch [326/451] | Loss: 0.6141\n",
      "Epoch [6] | Training Batch [351/451] | Loss: 0.6294\n",
      "Epoch [6] | Training Batch [351/451] | Loss: 0.6040\n",
      "Epoch [6] | Training Batch [376/451] | Loss: 0.6163\n",
      "Epoch [6] | Training Batch [376/451] | Loss: 0.6186\n",
      "Epoch [6] | Training Batch [401/451] | Loss: 0.6183\n",
      "Epoch [6] | Training Batch [401/451] | Loss: 0.6207\n",
      "Epoch [6] | Training Batch [426/451] | Loss: 0.6159\n",
      "Epoch [6] | Training Batch [426/451] | Loss: 0.6079\n",
      "Epoch [6] | Training Batch [451/451] | Loss: 0.6258\n",
      "Epoch [6] | Training Batch [451/451] | Loss: 0.6206\n",
      "Epoch [6] | Validation Batch [1/451] | Loss: 158.0371\n",
      "Epoch [6] | Validation Batch [1/451] | Loss: 155.1389\n",
      "Epoch [6] | Validation Batch [26/451] | Loss: 4123.6061\n",
      "Epoch [6] | Validation Batch [26/451] | Loss: 4082.8267\n",
      "Train Acc: 0.8293 | Val Acc: 0.8258\n",
      "Saved model with best performance so far.\n",
      "Epoch 7:\n",
      "Train Acc: 0.8129 | Val Acc: 0.8128\n",
      "Saved model with best performance so far.\n",
      "Epoch 7:\n",
      "Epoch [7] | Training Batch [1/451] | Loss: 0.6207\n",
      "Epoch [7] | Training Batch [1/451] | Loss: 0.6094\n",
      "Epoch [7] | Training Batch [26/451] | Loss: 0.6157\n",
      "Epoch [7] | Training Batch [26/451] | Loss: 0.6110\n",
      "Epoch [7] | Training Batch [51/451] | Loss: 0.6182\n",
      "Epoch [7] | Training Batch [51/451] | Loss: 0.6206\n",
      "Epoch [7] | Training Batch [76/451] | Loss: 0.6155\n",
      "Epoch [7] | Training Batch [76/451] | Loss: 0.6166\n",
      "Epoch [7] | Training Batch [101/451] | Loss: 0.6175\n",
      "Epoch [7] | Training Batch [101/451] | Loss: 0.6061\n",
      "Epoch [7] | Training Batch [126/451] | Loss: 0.6144\n",
      "Epoch [7] | Training Batch [126/451] | Loss: 0.5963\n",
      "Epoch [7] | Training Batch [151/451] | Loss: 0.6220\n",
      "Epoch [7] | Training Batch [151/451] | Loss: 0.6036\n",
      "Epoch [7] | Training Batch [176/451] | Loss: 0.6126\n",
      "Epoch [7] | Training Batch [176/451] | Loss: 0.6095\n",
      "Epoch [7] | Training Batch [201/451] | Loss: 0.6122\n",
      "Epoch [7] | Training Batch [201/451] | Loss: 0.6057\n",
      "Epoch [7] | Training Batch [226/451] | Loss: 0.6122\n",
      "Epoch [7] | Training Batch [226/451] | Loss: 0.6022\n",
      "Epoch [7] | Training Batch [251/451] | Loss: 0.6017\n",
      "Epoch [7] | Training Batch [251/451] | Loss: 0.5818\n",
      "Epoch [7] | Training Batch [276/451] | Loss: 0.6031\n",
      "Epoch [7] | Training Batch [276/451] | Loss: 0.6041\n",
      "Epoch [7] | Training Batch [301/451] | Loss: 0.6142\n",
      "Epoch [7] | Training Batch [301/451] | Loss: 0.6058\n",
      "Epoch [7] | Training Batch [326/451] | Loss: 0.6110\n",
      "Epoch [7] | Training Batch [326/451] | Loss: 0.6100\n",
      "Epoch [7] | Training Batch [351/451] | Loss: 0.6126\n",
      "Epoch [7] | Training Batch [351/451] | Loss: 0.6101\n",
      "Epoch [7] | Training Batch [376/451] | Loss: 0.6025\n",
      "Epoch [7] | Training Batch [376/451] | Loss: 0.6028\n",
      "Epoch [7] | Training Batch [401/451] | Loss: 0.6080\n",
      "Epoch [7] | Training Batch [401/451] | Loss: 0.6025\n",
      "Epoch [7] | Training Batch [426/451] | Loss: 0.6149\n",
      "Epoch [7] | Training Batch [426/451] | Loss: 0.5899\n",
      "Epoch [7] | Training Batch [451/451] | Loss: 0.6039\n",
      "Epoch [7] | Validation Batch [1/451] | Loss: 155.2450\n",
      "Epoch [7] | Training Batch [451/451] | Loss: 0.6083\n",
      "Epoch [7] | Validation Batch [1/451] | Loss: 152.2382\n",
      "Epoch [7] | Validation Batch [26/451] | Loss: 4057.1345\n",
      "Train Acc: 0.8327 | Val Acc: 0.8348\n",
      "Saved model with best performance so far.\n",
      "Epoch 8:\n",
      "Epoch [8] | Training Batch [1/451] | Loss: 0.6154\n",
      "Epoch [7] | Validation Batch [26/451] | Loss: 4013.3349\n",
      "Train Acc: 0.8164 | Val Acc: 0.8133\n",
      "Saved model with best performance so far.\n",
      "Epoch 8:\n",
      "Epoch [8] | Training Batch [1/451] | Loss: 0.5994\n",
      "Epoch [8] | Training Batch [26/451] | Loss: 0.6069\n",
      "Epoch [8] | Training Batch [26/451] | Loss: 0.5996\n",
      "Epoch [8] | Training Batch [51/451] | Loss: 0.5992\n",
      "Epoch [8] | Training Batch [51/451] | Loss: 0.5996\n",
      "Epoch [8] | Training Batch [76/451] | Loss: 0.6075\n",
      "Epoch [8] | Training Batch [76/451] | Loss: 0.5979\n",
      "Epoch [8] | Training Batch [101/451] | Loss: 0.6014\n",
      "Epoch [8] | Training Batch [101/451] | Loss: 0.5906\n",
      "Epoch [8] | Training Batch [126/451] | Loss: 0.6016\n",
      "Epoch [8] | Training Batch [126/451] | Loss: 0.6073\n",
      "Epoch [8] | Training Batch [151/451] | Loss: 0.5938\n",
      "Epoch [8] | Training Batch [151/451] | Loss: 0.6025\n",
      "Epoch [8] | Training Batch [176/451] | Loss: 0.6108\n",
      "Epoch [8] | Training Batch [176/451] | Loss: 0.5903\n",
      "Epoch [8] | Training Batch [201/451] | Loss: 0.6100\n",
      "Epoch [8] | Training Batch [201/451] | Loss: 0.5957\n",
      "Epoch [8] | Training Batch [226/451] | Loss: 0.6061\n",
      "Epoch [8] | Training Batch [226/451] | Loss: 0.5937\n",
      "Epoch [8] | Training Batch [251/451] | Loss: 0.6013\n",
      "Epoch [8] | Training Batch [251/451] | Loss: 0.6009\n",
      "Epoch [8] | Training Batch [276/451] | Loss: 0.6029\n",
      "Epoch [8] | Training Batch [276/451] | Loss: 0.5888\n",
      "Epoch [8] | Training Batch [301/451] | Loss: 0.6134\n",
      "Epoch [8] | Training Batch [301/451] | Loss: 0.5885\n",
      "Epoch [8] | Training Batch [326/451] | Loss: 0.6060\n",
      "Epoch [8] | Training Batch [326/451] | Loss: 0.5925\n",
      "Epoch [8] | Training Batch [351/451] | Loss: 0.6061\n",
      "Epoch [8] | Training Batch [351/451] | Loss: 0.5940\n",
      "Epoch [8] | Training Batch [376/451] | Loss: 0.6004\n",
      "Epoch [8] | Training Batch [376/451] | Loss: 0.6006\n",
      "Epoch [8] | Training Batch [401/451] | Loss: 0.6011\n",
      "Epoch [8] | Training Batch [401/451] | Loss: 0.5867\n",
      "Epoch [8] | Training Batch [426/451] | Loss: 0.5961\n",
      "Epoch [8] | Training Batch [426/451] | Loss: 0.6037\n",
      "Epoch [8] | Training Batch [451/451] | Loss: 0.6185\n",
      "Epoch [8] | Validation Batch [1/451] | Loss: 152.6613\n",
      "Epoch [8] | Training Batch [451/451] | Loss: 0.6169\n",
      "Epoch [8] | Validation Batch [1/451] | Loss: 149.3692\n",
      "Epoch [8] | Validation Batch [26/451] | Loss: 3991.1764\n",
      "Train Acc: 0.8374 | Val Acc: 0.8425\n",
      "Saved model with best performance so far.\n",
      "Epoch 9:\n",
      "Epoch [8] | Validation Batch [26/451] | Loss: 3946.9239\n",
      "Train Acc: 0.8194 | Val Acc: 0.8157\n",
      "Saved model with best performance so far.\n",
      "Epoch 9:\n",
      "Epoch [9] | Training Batch [1/451] | Loss: 0.6094\n",
      "Epoch [9] | Training Batch [1/451] | Loss: 0.5936\n",
      "Epoch [9] | Training Batch [26/451] | Loss: 0.6012\n",
      "Epoch [9] | Training Batch [26/451] | Loss: 0.5814\n",
      "Epoch [9] | Training Batch [51/451] | Loss: 0.5988\n",
      "Epoch [9] | Training Batch [51/451] | Loss: 0.5942\n",
      "Epoch [9] | Training Batch [76/451] | Loss: 0.6010\n",
      "Epoch [9] | Training Batch [76/451] | Loss: 0.6015\n",
      "Epoch [9] | Training Batch [101/451] | Loss: 0.5920\n",
      "Epoch [9] | Training Batch [101/451] | Loss: 0.5913\n",
      "Epoch [9] | Training Batch [126/451] | Loss: 0.5896\n",
      "Epoch [9] | Training Batch [126/451] | Loss: 0.5899\n",
      "Epoch [9] | Training Batch [151/451] | Loss: 0.5900\n",
      "Epoch [9] | Training Batch [151/451] | Loss: 0.5863\n",
      "Epoch [9] | Training Batch [176/451] | Loss: 0.5980\n",
      "Epoch [9] | Training Batch [176/451] | Loss: 0.5899\n",
      "Epoch [9] | Training Batch [201/451] | Loss: 0.6041\n",
      "Epoch [9] | Training Batch [201/451] | Loss: 0.6029\n",
      "Epoch [9] | Training Batch [226/451] | Loss: 0.5894\n",
      "Epoch [9] | Training Batch [226/451] | Loss: 0.5818\n",
      "Epoch [9] | Training Batch [251/451] | Loss: 0.5958\n",
      "Epoch [9] | Training Batch [251/451] | Loss: 0.5884\n",
      "Epoch [9] | Training Batch [276/451] | Loss: 0.5977\n",
      "Epoch [9] | Training Batch [276/451] | Loss: 0.5934\n",
      "Epoch [9] | Training Batch [301/451] | Loss: 0.5921\n",
      "Epoch [9] | Training Batch [301/451] | Loss: 0.5780\n",
      "Epoch [9] | Training Batch [326/451] | Loss: 0.5842\n",
      "Epoch [9] | Training Batch [326/451] | Loss: 0.5878\n",
      "Epoch [9] | Training Batch [351/451] | Loss: 0.5940\n",
      "Epoch [9] | Training Batch [351/451] | Loss: 0.5835\n",
      "Epoch [9] | Training Batch [376/451] | Loss: 0.5889\n",
      "Epoch [9] | Training Batch [376/451] | Loss: 0.5974\n",
      "Epoch [9] | Training Batch [401/451] | Loss: 0.6074\n",
      "Epoch [9] | Training Batch [401/451] | Loss: 0.5907\n",
      "Epoch [9] | Training Batch [426/451] | Loss: 0.5875\n",
      "Epoch [9] | Training Batch [426/451] | Loss: 0.5818\n",
      "Epoch [9] | Training Batch [451/451] | Loss: 0.5918\n",
      "Epoch [9] | Training Batch [451/451] | Loss: 0.5847\n",
      "Epoch [9] | Validation Batch [1/451] | Loss: 150.4319\n",
      "Epoch [9] | Validation Batch [1/451] | Loss: 146.8388\n",
      "Epoch [9] | Validation Batch [26/451] | Loss: 3886.1495\n",
      "Epoch [9] | Validation Batch [26/451] | Loss: 3937.0762\n",
      "Train Acc: 0.8215 | Val Acc: 0.8209\n",
      "Train Acc: 0.8437 | Val Acc: 0.8399\n",
      "Epoch 10:\n",
      "Saved model with best performance so far.\n",
      "Epoch 10:\n",
      "Epoch [10] | Training Batch [1/451] | Loss: 0.5790\n",
      "Epoch [10] | Training Batch [1/451] | Loss: 0.5858\n",
      "Epoch [10] | Training Batch [26/451] | Loss: 0.5785\n",
      "Epoch [10] | Training Batch [26/451] | Loss: 0.5900\n",
      "Epoch [10] | Training Batch [51/451] | Loss: 0.5758\n",
      "Epoch [10] | Training Batch [51/451] | Loss: 0.5973\n",
      "Epoch [10] | Training Batch [76/451] | Loss: 0.5781\n",
      "Epoch [10] | Training Batch [76/451] | Loss: 0.5788\n",
      "Epoch [10] | Training Batch [101/451] | Loss: 0.5804\n",
      "Epoch [10] | Training Batch [101/451] | Loss: 0.5910\n",
      "Epoch [10] | Training Batch [126/451] | Loss: 0.5851\n",
      "Epoch [10] | Training Batch [126/451] | Loss: 0.5897\n",
      "Epoch [10] | Training Batch [151/451] | Loss: 0.5822\n",
      "Epoch [10] | Training Batch [151/451] | Loss: 0.5854\n",
      "Epoch [10] | Training Batch [176/451] | Loss: 0.5802\n",
      "Epoch [10] | Training Batch [176/451] | Loss: 0.5755\n",
      "Epoch [10] | Training Batch [201/451] | Loss: 0.5859\n",
      "Epoch [10] | Training Batch [201/451] | Loss: 0.5961\n",
      "Epoch [10] | Training Batch [226/451] | Loss: 0.5696\n",
      "Epoch [10] | Training Batch [226/451] | Loss: 0.6013\n",
      "Epoch [10] | Training Batch [251/451] | Loss: 0.5660\n",
      "Epoch [10] | Training Batch [251/451] | Loss: 0.5797\n",
      "Epoch [10] | Training Batch [276/451] | Loss: 0.5842\n",
      "Epoch [10] | Training Batch [276/451] | Loss: 0.5818\n",
      "Epoch [10] | Training Batch [301/451] | Loss: 0.5700\n",
      "Epoch [10] | Training Batch [301/451] | Loss: 0.5732\n",
      "Epoch [10] | Training Batch [326/451] | Loss: 0.5712\n",
      "Epoch [10] | Training Batch [326/451] | Loss: 0.5776\n",
      "Epoch [10] | Training Batch [351/451] | Loss: 0.5805\n",
      "Epoch [10] | Training Batch [351/451] | Loss: 0.5647\n",
      "Epoch [10] | Training Batch [376/451] | Loss: 0.5799\n",
      "Epoch [10] | Training Batch [376/451] | Loss: 0.5909\n",
      "Epoch [10] | Training Batch [401/451] | Loss: 0.5621\n",
      "Epoch [10] | Training Batch [401/451] | Loss: 0.5841\n",
      "Epoch [10] | Training Batch [426/451] | Loss: 0.5606\n",
      "Epoch [10] | Training Batch [426/451] | Loss: 0.5715\n",
      "Epoch [10] | Training Batch [451/451] | Loss: 0.5671\n",
      "Epoch [10] | Validation Batch [1/451] | Loss: 144.2476\n",
      "Epoch [10] | Training Batch [451/451] | Loss: 0.5916\n",
      "Epoch [10] | Validation Batch [1/451] | Loss: 148.3605\n",
      "Epoch [10] | Validation Batch [26/451] | Loss: 3826.9946\n",
      "Train Acc: 0.8235 | Val Acc: 0.8196\n",
      "Training time for Model 1: 5039.91 seconds\n",
      "Epoch [10] | Validation Batch [26/451] | Loss: 3883.6949\n",
      "Train Acc: 0.8420 | Val Acc: 0.8422\n",
      "Training time for Model 2: 5052.49 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch.cuda\n",
    "import threading\n",
    "\n",
    "stream1 = torch.cuda.Stream()\n",
    "stream2 = torch.cuda.Stream()\n",
    "\n",
    "def train_model1():\n",
    "    model = models.shufflenet_v2_x0_5(weights=models.ShuffleNet_V2_X0_5_Weights.DEFAULT).cuda()\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    #Freeze all layers except the last FC layer\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.fc=nn.Linear(1024, 2)\n",
    "    model.fc.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.cuda.stream(stream1):\n",
    "        train_and_val(model, optimizer, loss_function, 10, \"./results/shufflenet_v2_x0_5.txt\", \"./shufflenet_v2_x0_5.pth\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Training time for Model 1: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "def train_model2():\n",
    "    model = models.shufflenet_v2_x1_0(weights=models.ShuffleNet_V2_X1_0_Weights.DEFAULT).cuda()\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    #Freeze all layers except the last FC layer\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.fc=nn.Linear(1024, 2)\n",
    "    model.fc.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.cuda.stream(stream2):\n",
    "        train_and_val(model, optimizer, loss_function, 10, \"./results/shufflenet_v2_x1_0.txt\", \"./shufflenet_v2_x1_0.pth\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Training time for Model 2: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "thread1 = threading.Thread(target=train_model1)\n",
    "thread2 = threading.Thread(target=train_model2)\n",
    "\n",
    "# Start both threads\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "# Wait for both threads to finish\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "\n",
    "# Synchronize streams\n",
    "stream1.synchronize()\n",
    "stream2.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.24s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import pyvww\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "training_set = pyvww.pytorch.VisualWakeWordsClassification(\n",
    "    root=\"./visualwakewords/path-to-COCO-dataset/train2014\",\n",
    "    annFile=\"./visualwakewords/new-path-to-visualwakewords-dataset/annotations/instances_train.json\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "validation_set = pyvww.pytorch.VisualWakeWordsClassification(\n",
    "    root=\"./visualwakewords/path-to-COCO-dataset/train2014\",\n",
    "    annFile=\"./visualwakewords/new-path-to-visualwakewords-dataset/annotations/instances_val.json\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.DEFAULT).cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[1] = torch.nn.Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "start_time = time.time()\n",
    "train_and_val(model, optimizer, loss_function, 10, \"./results/squeezenet1_1.txt\", \"./squeezenet1_1.pth\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training time for Model 1: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bonus_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
